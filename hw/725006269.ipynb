{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 1:  Retrieval Models: Boolean + Vector Space\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Thursday, February 2 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as lastname_firstinitial_hw#.ipynb. For example, my homework submission would be: caverlee_j_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Friday, February 5 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We provide the dataset, [southpark_scripts.zip](https://www.dropbox.com/s/6rzfsbn97s8vwof/southpark_scripts.zip), which includes scripts for episodes of the first twenty seasons of the TV show South Park. You will build an inverted index over these scripts where each episode should be treated as a single document. There are 277 episodes (documents) to index and search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (20 points) Part 1: Parsing\n",
    "\n",
    "First, you should tokenize documents using **whitespaces and punctuations as delimiters** but do not remove stop words. Your parser needs to also provide the following two confgiuration options:\n",
    "* Case-folding\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration options\n",
    "use_stemming = False # or false\n",
    "use_casefolding = False # or false\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your parser function here. It will take the two option variables above as the parameters\n",
    "# add cells as needed to organize your code\n",
    "import os\n",
    "import string\n",
    "import urllib2\n",
    "import glob\n",
    "from string import punctuation\n",
    "txt_files = 'southpark_scripts/'\n",
    "dict_word = {}\n",
    "ps = PorterStemmer()\n",
    "def parser(use_stemming, use_casefolding):\n",
    "    for txt_file in os.listdir(txt_files):\n",
    "        with open(txt_files + txt_file) as f:\n",
    "            for line in f:\n",
    "                for word in re.split('\\W+', line):\n",
    "                    if use_stemming:\n",
    "                        word = ps.stem(word)\n",
    "                    if use_casefolding:\n",
    "                        word = word.lower()\n",
    "                    if word not in dict_word:\n",
    "                        dict_word[word] = 1\n",
    "                    else:\n",
    "                        dict_word[word] = dict_word[word] + 1\n",
    "        \n",
    "    return dict_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29551\n"
     ]
    }
   ],
   "source": [
    "print len(parser(use_stemming,use_casefolding).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* Stemming + Casefolding       = 17138\n",
    "* Stemming + No Casefolding    = 22222\n",
    "* No Stemming + Casefolding    = 23807\n",
    "* No Stemming + No Casefolding = 29551\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 2: Boolean Retrieval\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to  support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **great again**, your search engine should treat it as **great** AND **again**.\n",
    "\n",
    "Example queries:\n",
    "* Rednecks\n",
    "* Troll Trace\n",
    "* Respect my authority\n",
    "* Respect my authoritah\n",
    "* Respected my authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the index here\n",
    "import collections\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "ps = PorterStemmer()\n",
    "txt_files = 'southpark_scripts/'\n",
    "dict_retrieval = collections.defaultdict(set)\n",
    "for txt_file in os.listdir(txt_files):\n",
    "    with open(txt_files + txt_file) as f:\n",
    "        for line in f:\n",
    "            for word in re.split('\\W+', line):\n",
    "                word = ps.stem(word)\n",
    "                word = word.lower()\n",
    "                \n",
    "                dict_retrieval[word].add(txt_file)\n",
    "                \n",
    "\n",
    "# add cells as needed to organize your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search:respect my authorities\n",
      "set(['1610.txt', '0304.txt', '1204.txt', '0503.txt', '0407.txt', '1402.txt', '2004.txt', '0904.txt', '0305.txt', '0314.txt'])\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search:')\n",
    "# search for the input using your index and print out ids of matching documents\n",
    "search_text = search_text.split()\n",
    "new_query = []\n",
    "for word in search_text:\n",
    "    word = ps.stem(word)\n",
    "    word = word.lower()\n",
    "    new_query.append(word)\n",
    "res = dict_retrieval[new_query[0]]\n",
    "for word in new_query[1:]:\n",
    "    res = res & dict_retrieval[word]\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "When we use stemming and casefolding, is the result different from the result when we do not use them? Do you find cases where you prefer stemming? Or not? Or cases where you prefer casefolding? Or Not? Write down your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we remove casefolding, we get different results for queries in upper case and lower case. For example, \"Troll Trace\" and \"troll trace\" give difference set of files. Troll Trace is found in 5 files while troll trace is found in one file. With case folding, we get six files.\n",
    "\n",
    "Similarly if we remove stemming, for a query \"respect my authorities\", we just get one result. But with stemming, we get 10 results. \n",
    "\n",
    "In my opinion, we should prefer stemming and casefolding as it gives more consistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Phrase Queries\n",
    "\n",
    "Your search engine needs to also (optionally) support phrase queries of arbitrary length. Use quotes in a query to tell your search engine this is a phrase query. Again, we don't explicitly type AND in queries and never use OR, NOT, or parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import collections\n",
    "import os\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "ps = PorterStemmer()\n",
    "txt_files = 'southpark_scripts/'\n",
    "dict_retrieval = collections.defaultdict(set)\n",
    "for txt_file in os.listdir(txt_files):\n",
    "    with open(txt_files + txt_file) as f:\n",
    "        for line in f:\n",
    "            for word in re.split('\\W+', line):\n",
    "                word = word.decode('ascii','ignore')\n",
    "                new_word = ''.join(ch for ch in word if ch not in punctuation)\n",
    "                n = ps.stem(new_word.lower())\n",
    "                \n",
    "                dict_retrieval[n].add(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search (Phrase Query:hjhjdsfs\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search (Phrase Query:')\n",
    "# search for the input using your index and print out ids of matching documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 3: Ranked Retrieval\n",
    "\n",
    "In this part, your job is to support queries over an index that you build. This time you will use the vector space model plus cosine similarity to rank documents.\n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TFIDF scores. That is, use the log-weighted term frequency $1+log(tf)$; and the log-weighted inverse document frequency $log(\\frac{N}{df})$. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For a given query, you should rank all the 277 documents but you only need to output the top-5 documents (i.e. document ids) plus the cosine score of each of these documents. For example:\n",
    "\n",
    "* result1 - score1\n",
    "* result2 - score2\n",
    "* result3 - score3\n",
    "* result4 - score4\n",
    "* result5 - score5\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Search:king of westeros\n",
      "1607.txt - 0.113541031408\n",
      "1708.txt - 0.0333568778169\n",
      "0502.txt - 0.0267071738338\n",
      "0614.txt - 0.0261273049258\n",
      "1704.txt - 0.024962492335\n"
     ]
    }
   ],
   "source": [
    "# build the vector space index here\n",
    "import os\n",
    "import string\n",
    "import urllib2\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import collections\n",
    "import operator\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter\n",
    "ps = PorterStemmer()\n",
    "txt_files = 'southpark_scripts/'\n",
    "dict_ranked_word = collections.defaultdict(list)\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for txt_file in os.listdir(txt_files):\n",
    "    with open(txt_files + txt_file) as f:\n",
    "        for line in f:\n",
    "            for word in re.split('\\W+', line):\n",
    "               \n",
    "                word = ps.stem(word)\n",
    "                \n",
    "                word = word.lower()\n",
    "                dict_ranked_word[word].append(txt_file)\n",
    "\n",
    "def calculateNorm(dictionary):\n",
    "    result = 0.0\n",
    "    for key, value in dictionary.iteritems():\n",
    "        result = result + value * value\n",
    "    result = math.sqrt(result)\n",
    "    if result != 0:\n",
    "        for key, value in dictionary.iteritems():\n",
    "            dictionary[key] = float(value/result)\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "search_text = raw_input('Ranked Search:')\n",
    "search_text = re.split('\\W+', search_text)\n",
    "new_search_text = []\n",
    "for word in search_text:\n",
    "    word = ps.stem(word)\n",
    "    word = word.lower()\n",
    "    new_search_text.append(word)\n",
    "\n",
    "query_vector_dict = {}\n",
    "for x in new_search_text:\n",
    "    if x in query_vector_dict:\n",
    "        query_vector_dict[x] = query_vector_dict[x] + 1\n",
    "    else:\n",
    "        query_vector_dict[x] = 1\n",
    "\n",
    "result_scores = {}\n",
    "for txt_file in os.listdir(txt_files):\n",
    "    document_dict = {}\n",
    "    res = 0.0\n",
    "    for key, value in dict_ranked_word.iteritems():\n",
    "        if key.encode('utf-8') != '':\n",
    "            if txt_file in dict_ranked_word[key]:\n",
    "                tf = dict_ranked_word[key].count(txt_file)\n",
    "                wf = 1+math.log10(tf)\n",
    "            else:\n",
    "                tf = 0\n",
    "                wf = 0\n",
    "            doc_freq = len(set(dict_ranked_word[key]))\n",
    "            inv_doc_freq = math.log10(277/doc_freq)\n",
    "            wfidf = wf * inv_doc_freq\n",
    "            document_dict[key.encode('utf-8')] = wfidf\n",
    "    normalised_document_dict = {}\n",
    "    normalised_document_dict = calculateNorm(document_dict)\n",
    "    normalised_query_dict = {}\n",
    "    normalised_query_dict = calculateNorm(query_vector_dict)\n",
    "    for key, value in normalised_query_dict.iteritems():\n",
    "        if key in normalised_document_dict:\n",
    "            res = res + normalised_query_dict[key] * normalised_document_dict[key]\n",
    "        else:\n",
    "            res = res\n",
    "    result_scores[txt_file] = res\n",
    "d = Counter(result_scores)\n",
    "for key, value in d.most_common(5):\n",
    "    print key, '-', value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grader will randomly pick 5-10 queries to test your program. You are welcome to discuss the results returned by your search engine with others on Piazza."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
